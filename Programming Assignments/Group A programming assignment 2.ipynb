{"cells":[{"cell_type":"markdown","metadata":{"id":"lLmBNlsef2LZ"},"source":["# Group A Programming Assignment 2 Report\n","\n","**Authors:** Aidnel M Martínez Meléndez, Alex Demel Pacheco, Edgar J Suárez-Colón  \n","**Course:** ICOM5015-001D Artificial Intelligence"]},{"cell_type":"markdown","metadata":{"id":"3DHY8XcIf2Lc"},"source":["## Task Division\n","\n","| Task            | GroUp Member      |\n","|:-----------------:|:-------------------:|\n","| Programming     | Alex Demel        |\n","| Debugging       | Edgar Suarez      |\n","| Report Writing  | Aidnel Martínez   |\n","| Report Editing  | Edgar Suarez      |\n","| Video Scripting | Aidnel Martínez   |\n","| Video Editing   | Alex Demel        |"]},{"cell_type":"markdown","metadata":{"id":"46b-5JFNf2Ld"},"source":["## I. Introduction\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This project presents a study into the various types of intelligent agents, with the aim of deepening our understanding through practical application. In pursuit of this goal, we have embarked on a project to develop modular simulators capable of measuring agent performance, thereby facilitating a comprehensive comparison of different agents across a spectrum of conditions. This endeavor seeks not only to observe their behaviors but also to ascertain the most appropriate agent for each specific task. By evaluating and analyzing each agent's behavior and performance, this project will address key questions for each agent, underscoring the significance of selecting the optimal agent for problem-solving. Such a choice is critical as future computer engineers, since the efficacy of a solution and its success rate are directly impacted by the appropriateness of the agent deployed."]},{"cell_type":"markdown","metadata":{"id":"p8nIKStLf2Ld"},"source":["## II. Excercises and Experiments"]},{"cell_type":"markdown","metadata":{"id":"rh-2uMA6f2Le"},"source":["#### Excercise 2.11\n","The code below implements a performance-measuring environment simulator for a simple vacuum-cleaner world with only two spaces (A, B) both initially dirty. The simulator was designed to be modular, and allows the user to easily change the environment size, shape, and dirt placement."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yp85pSIDf2Le"},"outputs":[],"source":["class SimpleVacuumCleanerWorld:\n","    def __init__(self, dirt_placement):\n","        self.dirt_placement = dirt_placement\n","\n","    def is_valid_position(self, position):\n","        return 0 <= position < len(self.dirt_placement)\n","\n","    def check_space(self, position):\n","        if self.is_valid_position(position):\n","            return self.dirt_placement[position]\n","        else:\n","            if position < 0:\n","                return \"Left-Boundary\"\n","            elif position >= len(self.dirt_placement):\n","                return \"Right-Boundary\"\n","\n","    def clean(self, position):\n","        if self.is_valid_position(position) and self.dirt_placement[position] == \"Dirty\":\n","            self.dirt_placement[position] = \"Clean\"\n","\n","class EnvironmentSimulator:\n","    def __init__(self, agent, starting_position, environment):\n","        self.agent = agent\n","        self.agent_position = starting_position\n","        self.environment = environment\n","        self.agent_performance = 0\n","\n","        print(f\"Initial Agent Position: {self.agent_position}, Performance: {self.agent_performance}, Environment state:\")\n","        print(\"\".join(f\"[{col}]\" if x == self.agent_position else f\" {col} \" for x, col in enumerate(self.environment.dirt_placement)))\n","\n","    def execute_action(self, action):\n","        direction_map = {\"Left\": -1, \"Right\": 1}\n","        dx = direction_map.get(action, 0)\n","        new_position = self.agent_position + dx\n","\n","        if action == \"Suck\":\n","            if self.environment.check_space(self.agent_position) == \"Dirty\":\n","                self.environment.clean(self.agent_position)\n","                self.agent_performance += 10\n","            else:\n","                self.agent_performance -= 1\n","        elif action in direction_map:\n","            self.agent_performance -= 1\n","            percept = self.environment.check_space(new_position)\n","            if percept not in [\"Left-Boundary\", \"Right-Boundary\"]:\n","                self.agent_position = new_position\n","            else:\n","                self.execute_action(self.agent.action(percept))\n","\n","    def step(self, step):\n","        percept = self.environment.check_space(self.agent_position)\n","        action = self.agent.action(percept)\n","        self.execute_action(action)\n","        print(f\"Step: {step}, Agent Position: {self.agent_position}, Performance: {self.agent_performance}, Environment state:\")\n","        print(\"\".join(f\"[{col}]\" if x == self.agent_position else f\" {col} \" for x, col in enumerate(self.environment.dirt_placement)))\n","\n","dirt_placement = [\"Dirty\", \"Dirty\"]\n","simple_vacuum_world = SimpleVacuumCleanerWorld(dirt_placement)"]},{"cell_type":"markdown","metadata":{"id":"_pVJqbc1f2Lf"},"source":["---\n","\n","To test the environment, we can introduce a basic reflex agent to the simulation and observe its performance, evaluated by the total number of spaces it successfully cleans (+10), minus the number of movements it makes (-1). The design for this agent is based on the code and pseudocode provided by [1][2]."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KetKKEekf2Lf","outputId":"5dc423bf-b4a4-413f-a54c-b3d3f2461d63"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial Agent Position: 0, Performance: 0, Environment state:\n","[Dirty] Dirty \n","Step: 0, Agent Position: 0, Performance: 10, Environment state:\n","[Clean] Dirty \n","Step: 1, Agent Position: 1, Performance: 9, Environment state:\n"," Clean [Dirty]\n","Step: 2, Agent Position: 1, Performance: 19, Environment state:\n"," Clean [Clean]\n"]}],"source":["class SimpleReflexAgent:\n","    def __init__(self):\n","        self.rules = {\n","            \"Dirty\": \"Suck\",\n","            \"Clean\": \"Move\",\n","            \"Left-Boundary\": \"Right\",\n","            \"Right-Boundary\": \"Left\",\n","            \"Direction\": \"Right\"\n","        }\n","\n","    def rule_match(self, percept):\n","        return self.rules[percept]\n","\n","    def action(self, percept):\n","        action = self.rule_match(percept)\n","\n","        if action == \"Suck\":\n","            return action\n","        elif action == \"Move\":\n","            return self.rules[\"Direction\"]\n","        elif action == \"Left\":\n","            self.rules[\"Direction\"] = \"Left\"\n","        elif action == \"Right\":\n","            self.rules[\"Direction\"] = \"Right\"\n","\n","        return self.rules[\"Direction\"]\n","\n","simple_agent = SimpleReflexAgent()\n","simulator = EnvironmentSimulator(simple_agent, 0, simple_vacuum_world)\n","\n","for step in range(3):\n","    simulator.step(step)"]},{"cell_type":"markdown","metadata":{"id":"KecDIxnmf2Lg"},"source":["**Experiment 1:**\n","\n","The results of the simulation show that the agent is capable of successfully navigating the environment and cleaning its spaces within three steps of the simulation, finishing with a final performance of 19.\n","\n","For further experimentation, we can modify various parameters such as the the agent's initial position, the environment's shape, dirt placement, and the number of steps in the simulation to observe how the agent performs under these altered conditions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4PCXEVZf2Lg","outputId":"429610b4-f9a8-4811-b00d-e4de9a1dc606"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial Agent Position: 3, Performance: 0, Environment state:\n"," Dirty  Dirty  Clean [Clean] Dirty  Clean  Dirty  Dirty  Clean \n","Step: 0, Agent Position: 4, Performance: -1, Environment state:\n"," Dirty  Dirty  Clean  Clean [Dirty] Clean  Dirty  Dirty  Clean \n","Step: 1, Agent Position: 4, Performance: 9, Environment state:\n"," Dirty  Dirty  Clean  Clean [Clean] Clean  Dirty  Dirty  Clean \n","Step: 2, Agent Position: 5, Performance: 8, Environment state:\n"," Dirty  Dirty  Clean  Clean  Clean [Clean] Dirty  Dirty  Clean \n","Step: 3, Agent Position: 6, Performance: 7, Environment state:\n"," Dirty  Dirty  Clean  Clean  Clean  Clean [Dirty] Dirty  Clean \n","Step: 4, Agent Position: 6, Performance: 17, Environment state:\n"," Dirty  Dirty  Clean  Clean  Clean  Clean [Clean] Dirty  Clean \n","Step: 5, Agent Position: 7, Performance: 16, Environment state:\n"," Dirty  Dirty  Clean  Clean  Clean  Clean  Clean [Dirty] Clean \n","Step: 6, Agent Position: 7, Performance: 26, Environment state:\n"," Dirty  Dirty  Clean  Clean  Clean  Clean  Clean [Clean] Clean \n","Step: 7, Agent Position: 8, Performance: 25, Environment state:\n"," Dirty  Dirty  Clean  Clean  Clean  Clean  Clean  Clean [Clean]\n","Step: 8, Agent Position: 7, Performance: 23, Environment state:\n"," Dirty  Dirty  Clean  Clean  Clean  Clean  Clean [Clean] Clean \n","Step: 9, Agent Position: 6, Performance: 22, Environment state:\n"," Dirty  Dirty  Clean  Clean  Clean  Clean [Clean] Clean  Clean \n","Step: 10, Agent Position: 5, Performance: 21, Environment state:\n"," Dirty  Dirty  Clean  Clean  Clean [Clean] Clean  Clean  Clean \n","Step: 11, Agent Position: 4, Performance: 20, Environment state:\n"," Dirty  Dirty  Clean  Clean [Clean] Clean  Clean  Clean  Clean \n","Step: 12, Agent Position: 3, Performance: 19, Environment state:\n"," Dirty  Dirty  Clean [Clean] Clean  Clean  Clean  Clean  Clean \n","Step: 13, Agent Position: 2, Performance: 18, Environment state:\n"," Dirty  Dirty [Clean] Clean  Clean  Clean  Clean  Clean  Clean \n","Step: 14, Agent Position: 1, Performance: 17, Environment state:\n"," Dirty [Dirty] Clean  Clean  Clean  Clean  Clean  Clean  Clean \n","Step: 15, Agent Position: 1, Performance: 27, Environment state:\n"," Dirty [Clean] Clean  Clean  Clean  Clean  Clean  Clean  Clean \n","Step: 16, Agent Position: 0, Performance: 26, Environment state:\n","[Dirty] Clean  Clean  Clean  Clean  Clean  Clean  Clean  Clean \n","Step: 17, Agent Position: 0, Performance: 36, Environment state:\n","[Clean] Clean  Clean  Clean  Clean  Clean  Clean  Clean  Clean \n","Step: 18, Agent Position: 1, Performance: 34, Environment state:\n"," Clean [Clean] Clean  Clean  Clean  Clean  Clean  Clean  Clean \n","Step: 19, Agent Position: 2, Performance: 33, Environment state:\n"," Clean  Clean [Clean] Clean  Clean  Clean  Clean  Clean  Clean \n","Step: 20, Agent Position: 3, Performance: 32, Environment state:\n"," Clean  Clean  Clean [Clean] Clean  Clean  Clean  Clean  Clean \n","Step: 21, Agent Position: 4, Performance: 31, Environment state:\n"," Clean  Clean  Clean  Clean [Clean] Clean  Clean  Clean  Clean \n","Step: 22, Agent Position: 5, Performance: 30, Environment state:\n"," Clean  Clean  Clean  Clean  Clean [Clean] Clean  Clean  Clean \n","Step: 23, Agent Position: 6, Performance: 29, Environment state:\n"," Clean  Clean  Clean  Clean  Clean  Clean [Clean] Clean  Clean \n","Step: 24, Agent Position: 7, Performance: 28, Environment state:\n"," Clean  Clean  Clean  Clean  Clean  Clean  Clean [Clean] Clean \n"]}],"source":["dirt_placement = [\"Dirty\", \"Dirty\", \"Clean\", \"Clean\", \"Dirty\", \"Clean\", \"Dirty\", \"Dirty\", \"Clean\"]\n","vacuum_world = SimpleVacuumCleanerWorld(dirt_placement)\n","\n","simple_agent = SimpleReflexAgent()\n","simulator = EnvironmentSimulator(simple_agent, 3, vacuum_world)\n","\n","for step in range(25):\n","    simulator.step(step)"]},{"cell_type":"markdown","metadata":{"id":"lJSNP34rf2Lg"},"source":["**Experiment 2:**\n","\n","Under the modified conditions, the agent is still able to navigate the environment and clean all its spaces, concluding with a final performance score of 28. However, it is observed that the agent persists in moving back and forth within the environment, even after the completion of its cleaning tasks. This behavior aligns with the expected operational pattern of a simple reflex agent [1], which operates without retaining any memory of the spaces it has previously visited.\n","\n","----"]},{"cell_type":"markdown","metadata":{"id":"CT5t4rnqf2Lg"},"source":["#### Excercise 2.14\n","The following code presents an modified version of the vacuum environment from the previous exercise, characterized by an element of uncertainty regarding its size, boundaries, and initial dirt placement. In this updated version, an agent is able to navigate not only left and right but also up and down, accommodating the expanded complexity of this modified environment. Furthermore, in this revised simulator, the agent's performance calculation now includes a penalty for the remaining \"Dirty\" spaces within the environment (-10)."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"lz2uGxKaf2Lg","executionInfo":{"status":"ok","timestamp":1709707627917,"user_tz":240,"elapsed":207,"user":{"displayName":"Alex Demel-Pacheco","userId":"13479886468262690947"}}},"outputs":[],"source":["class VacuumCleanerWorld:\n","    def __init__(self, dirt_placement):\n","        self.dirt_placement = dirt_placement\n","\n","    def is_valid_position(self, position):\n","        x, y = position\n","        if 0 <= y < len(self.dirt_placement):\n","            return 0 <= x < len(self.dirt_placement[y])\n","        return False\n","\n","    def check_space(self, position, action=None):\n","        x, y = position\n","        if (action == \"Left\" or action == \"Right\" or action == None) and 0 <= y < len(self.dirt_placement):\n","            if 0 <= x < len(self.dirt_placement[y]):\n","                return self.dirt_placement[y][x]\n","            else:\n","                if x < 0:\n","                    return \"Left-Boundary\"\n","                elif x >= len(self.dirt_placement[y]):\n","                    return \"Right-Boundary\"\n","        else:\n","            if y < 0:\n","                return \"Top-Boundary\"\n","            elif y >= len(self.dirt_placement):\n","                return \"Bottom-Boundary\"\n","            elif not self.is_valid_position(position) and action == \"Up\":\n","                return \"Top-Boundary\"\n","            elif not self.is_valid_position(position) and action == \"Down\":\n","                return \"Bottom-Boundary\"\n","\n","    def clean(self, position):\n","        if self.is_valid_position(position) and self.dirt_placement[position[1]][position[0]] == \"Dirty\":\n","            self.dirt_placement[position[1]][position[0]] = \"Clean\"\n","\n","class EnvironmentSimulator:\n","    def __init__(self, agent, starting_position, environment):\n","        self.agent = agent\n","        self.agent_position = starting_position\n","        self.environment = environment\n","        self.agent_performance = 0\n","\n","        print(f\"Initial Agent Position: {self.agent_position}, Performance: {self.agent_performance}, Environment state:\")\n","        for y, row in enumerate(self.environment.dirt_placement):\n","            row_str = \"\".join(f\"[{col}]\" if (x, y) == self.agent_position else f\" {col} \" for x, col in enumerate(row))\n","            print(row_str)\n","\n","    def execute_action(self, action):\n","        direction_map = {\"Left\": (-1, 0), \"Right\": (1, 0), \"Up\": (0, -1), \"Down\": (0, 1)}\n","        dx, dy = direction_map.get(action, (0, 0))\n","        new_position = (self.agent_position[0] + dx, self.agent_position[1] + dy)\n","\n","        if action == \"Suck\":\n","            if self.environment.check_space(self.agent_position) == \"Dirty\":\n","                self.environment.clean(self.agent_position)\n","                self.agent_performance += 10\n","            else:\n","                self.agent_performance -= 1\n","        elif action in direction_map:\n","            self.agent_performance -= 1\n","            percept = self.environment.check_space(new_position, action)\n","            if percept not in [\"Left-Boundary\", \"Right-Boundary\", \"Top-Boundary\", \"Bottom-Boundary\"]:\n","                self.agent_position = new_position\n","            else:\n","                self.execute_action(self.agent.action(percept))\n","\n","    def step(self, step):\n","        percept = self.environment.check_space(self.agent_position)\n","        action = self.agent.action(percept)\n","        self.execute_action(action)\n","        print(f\"Step: {step}, Agent Position: {self.agent_position}, Performance: {self.agent_performance}, Environment state:\")\n","        for y, row in enumerate(self.environment.dirt_placement):\n","            row_str = \"\".join(f\"[{col}]\" if (x, y) == self.agent_position else f\" {col} \" for x, col in enumerate(row))\n","            print(row_str)"]},{"cell_type":"markdown","metadata":{"id":"KkjDQWnzf2Lh"},"source":["1. Given the operational limitations of a simple reflex agent, which relies exclusively on condition-action rules, its effectiveness is expected to diminish in an unfamiliar environment [1]. The inherent design of such agents does not accommodate the variability of potential environmental configurations, leading to a probable scenario where the agent becomes entrapped in repetitive loops. Consequently, this may prevent the agent from conducting a comprehensive exploration of the environment, resulting in some areas remaining dirty.\n","\n","2. In contrast, a simple reflex agent equipped with a randomized decision function might demonstrate superior performance in comparison to its rule-bound counterpart. Integrating randomness breaks the chains of rigid, rule-based navigation, allowing the agent to comprehensively navigate the environment (if given enough time) [1]. To empirically validate this theory, simulations featuring both types of agents across a variety of environmental shapes will be conducted below, offering insight into the efficacy of randomness in enhancing exploratory tasks.\n","\n","**Note:** The agents below were developed using the code and pseudocode provided by [1][2]."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"22HF8Dzff2Lh"},"outputs":[],"source":["import random\n","\n","class SimpleReflexAgent:\n","    def __init__(self):\n","        self.rules = {\n","            \"Dirty\": \"Suck\",\n","            \"Clean\": \"Move\",\n","            \"Left-Boundary\": \"Up\",\n","            \"Right-Boundary\": \"Down\",\n","            \"Top-Boundary\": \"Right\",\n","            \"Bottom-Boundary\": \"Left\",\n","            \"Direction\": \"Right\"\n","        }\n","\n","    def rule_match(self, percept):\n","        return self.rules[percept]\n","\n","    def action(self, percept):\n","        action = self.rule_match(percept)\n","\n","        if action == \"Suck\":\n","            return action\n","        elif action == \"Move\":\n","            return self.rules[\"Direction\"]\n","        elif action == \"Left\":\n","            self.rules[\"Direction\"] = \"Left\"\n","        elif action == \"Right\":\n","            self.rules[\"Direction\"] = \"Right\"\n","        elif action == \"Up\":\n","            self.rules[\"Direction\"] = \"Up\"\n","        elif action == \"Down\":\n","            self.rules[\"Direction\"] = \"Down\"\n","\n","        return self.rules[\"Direction\"]\n","\n","class SimpleReflexRandomAgent:\n","    def __init__(self):\n","        self.rules = {\n","            \"Dirty\": \"Suck\",\n","            \"Clean\": \"Move\",\n","            \"Left-Boundary\": \"Up\",\n","            \"Right-Boundary\": \"Down\",\n","            \"Top-Boundary\": \"Right\",\n","            \"Bottom-Boundary\": \"Left\"\n","        }\n","\n","    def rule_match(self, percept):\n","        return self.rules[percept]\n","\n","    def action(self, percept):\n","        action = self.rule_match(percept)\n","\n","        if action == \"Move\":\n","            action = random.choice([\"Right\", \"Down\", \"Left\", \"Up\"])\n","\n","        return action"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aXWn5DQaf2Lh","outputId":"045ce537-0169-40dd-a1da-03fd49ab83c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial Agent Position: (0, 0), Performance: 0, Environment state:\n","[Dirty] Dirty  Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 0, Agent Position: (0, 0), Performance: 10, Environment state:\n","[Clean] Dirty  Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 1, Agent Position: (1, 0), Performance: 9, Environment state:\n"," Clean [Dirty] Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 2, Agent Position: (1, 0), Performance: 19, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 3, Agent Position: (2, 0), Performance: 18, Environment state:\n"," Clean  Clean [Clean]\n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 4, Agent Position: (1, 0), Performance: 15, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 5, Agent Position: (0, 0), Performance: 14, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 6, Agent Position: (1, 0), Performance: 11, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 7, Agent Position: (2, 0), Performance: 10, Environment state:\n"," Clean  Clean [Clean]\n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 8, Agent Position: (1, 0), Performance: 7, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 9, Agent Position: (0, 0), Performance: 6, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 10, Agent Position: (1, 0), Performance: 3, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 11, Agent Position: (2, 0), Performance: 2, Environment state:\n"," Clean  Clean [Clean]\n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 12, Agent Position: (1, 0), Performance: -1, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 13, Agent Position: (0, 0), Performance: -2, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 14, Agent Position: (1, 0), Performance: -5, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 15, Agent Position: (2, 0), Performance: -6, Environment state:\n"," Clean  Clean [Clean]\n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 16, Agent Position: (1, 0), Performance: -9, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 17, Agent Position: (0, 0), Performance: -10, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 18, Agent Position: (1, 0), Performance: -13, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 19, Agent Position: (2, 0), Performance: -14, Environment state:\n"," Clean  Clean [Clean]\n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 20, Agent Position: (1, 0), Performance: -17, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 21, Agent Position: (0, 0), Performance: -18, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 22, Agent Position: (1, 0), Performance: -21, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 23, Agent Position: (2, 0), Performance: -22, Environment state:\n"," Clean  Clean [Clean]\n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 24, Agent Position: (1, 0), Performance: -25, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 25, Agent Position: (0, 0), Performance: -26, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 26, Agent Position: (1, 0), Performance: -29, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 27, Agent Position: (2, 0), Performance: -30, Environment state:\n"," Clean  Clean [Clean]\n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 28, Agent Position: (1, 0), Performance: -33, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 29, Agent Position: (0, 0), Performance: -34, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 30, Agent Position: (1, 0), Performance: -37, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 31, Agent Position: (2, 0), Performance: -38, Environment state:\n"," Clean  Clean [Clean]\n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 32, Agent Position: (1, 0), Performance: -41, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 33, Agent Position: (0, 0), Performance: -42, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 34, Agent Position: (1, 0), Performance: -45, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Total steps: 35, Final performance: -95\n"]}],"source":["agent = SimpleReflexAgent()\n","environment = VacuumCleanerWorld([\n","    [\"Dirty\", \"Dirty\", \"Clean\"],\n","    [\"Clean\", \"Dirty\"],\n","    [\"Dirty\", \"Clean\", \"Dirty\", \"Dirty\"],\n","    [\"Clean\", \"Dirty\"]\n","])\n","\n","total_steps = 35\n","simulation = EnvironmentSimulator(agent, (0, 0), environment)\n","\n","for step in range(total_steps):\n","    simulation.step(step)\n","\n","simulation.agent_performance -= sum(space == \"Dirty\" for row in environment.dirt_placement for space in row) * 10\n","print(f\"Total steps: {total_steps}, Final performance: {simulation.agent_performance}\")"]},{"cell_type":"markdown","metadata":{"id":"iQrmqJ26f2Lh"},"source":["**Experiment 3:**\n","\n","As hypothesized, the simple reflex agent struggles to navigate the unknown environment, quickly becoming ensnared in a loop and failing to clean all the environment spaces, ultimately resulting in a final performance score of -95."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"csmzmTJef2Lh","outputId":"947e896d-bde8-459e-bbe4-b8279a99f907"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial Agent Position: (1, 1), Performance: 0, Environment state:\n"," Dirty  Dirty  Clean \n"," Clean [Dirty]\n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 0, Agent Position: (1, 1), Performance: 10, Environment state:\n"," Dirty  Dirty  Clean \n"," Clean [Clean]\n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 1, Agent Position: (0, 1), Performance: 9, Environment state:\n"," Dirty  Dirty  Clean \n","[Clean] Clean \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 2, Agent Position: (0, 2), Performance: 8, Environment state:\n"," Dirty  Dirty  Clean \n"," Clean  Clean \n","[Dirty] Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 3, Agent Position: (0, 2), Performance: 18, Environment state:\n"," Dirty  Dirty  Clean \n"," Clean  Clean \n","[Clean] Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 4, Agent Position: (0, 1), Performance: 16, Environment state:\n"," Dirty  Dirty  Clean \n","[Clean] Clean \n"," Clean  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 5, Agent Position: (1, 1), Performance: 15, Environment state:\n"," Dirty  Dirty  Clean \n"," Clean [Clean]\n"," Clean  Clean  Dirty  Dirty \n"," Clean  Dirty \n","Step: 6, Agent Position: (1, 2), Performance: 14, Environment state:\n"," Dirty  Dirty  Clean \n"," Clean  Clean \n"," Clean [Clean] Dirty  Dirty \n"," Clean  Dirty \n","Step: 7, Agent Position: (2, 2), Performance: 13, Environment state:\n"," Dirty  Dirty  Clean \n"," Clean  Clean \n"," Clean  Clean [Dirty] Dirty \n"," Clean  Dirty \n","Step: 8, Agent Position: (2, 2), Performance: 23, Environment state:\n"," Dirty  Dirty  Clean \n"," Clean  Clean \n"," Clean  Clean [Clean] Dirty \n"," Clean  Dirty \n","Step: 9, Agent Position: (1, 2), Performance: 21, Environment state:\n"," Dirty  Dirty  Clean \n"," Clean  Clean \n"," Clean [Clean] Clean  Dirty \n"," Clean  Dirty \n","Step: 10, Agent Position: (1, 3), Performance: 20, Environment state:\n"," Dirty  Dirty  Clean \n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean [Dirty]\n","Step: 11, Agent Position: (1, 3), Performance: 30, Environment state:\n"," Dirty  Dirty  Clean \n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean [Clean]\n","Step: 12, Agent Position: (0, 3), Performance: 29, Environment state:\n"," Dirty  Dirty  Clean \n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n","[Clean] Clean \n","Step: 13, Agent Position: (0, 2), Performance: 28, Environment state:\n"," Dirty  Dirty  Clean \n"," Clean  Clean \n","[Clean] Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 14, Agent Position: (1, 2), Performance: 27, Environment state:\n"," Dirty  Dirty  Clean \n"," Clean  Clean \n"," Clean [Clean] Clean  Dirty \n"," Clean  Clean \n","Step: 15, Agent Position: (1, 1), Performance: 26, Environment state:\n"," Dirty  Dirty  Clean \n"," Clean [Clean]\n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 16, Agent Position: (1, 2), Performance: 25, Environment state:\n"," Dirty  Dirty  Clean \n"," Clean  Clean \n"," Clean [Clean] Clean  Dirty \n"," Clean  Clean \n","Step: 17, Agent Position: (1, 1), Performance: 24, Environment state:\n"," Dirty  Dirty  Clean \n"," Clean [Clean]\n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 18, Agent Position: (1, 0), Performance: 23, Environment state:\n"," Dirty [Dirty] Clean \n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 19, Agent Position: (1, 0), Performance: 33, Environment state:\n"," Dirty [Clean] Clean \n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 20, Agent Position: (2, 0), Performance: 31, Environment state:\n"," Dirty  Clean [Clean]\n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 21, Agent Position: (1, 0), Performance: 28, Environment state:\n"," Dirty [Clean] Clean \n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 22, Agent Position: (2, 0), Performance: 26, Environment state:\n"," Dirty  Clean [Clean]\n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 23, Agent Position: (1, 0), Performance: 23, Environment state:\n"," Dirty [Clean] Clean \n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 24, Agent Position: (1, 1), Performance: 22, Environment state:\n"," Dirty  Clean  Clean \n"," Clean [Clean]\n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 25, Agent Position: (1, 0), Performance: 21, Environment state:\n"," Dirty [Clean] Clean \n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 26, Agent Position: (2, 0), Performance: 20, Environment state:\n"," Dirty  Clean [Clean]\n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 27, Agent Position: (1, 0), Performance: 18, Environment state:\n"," Dirty [Clean] Clean \n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 28, Agent Position: (0, 0), Performance: 17, Environment state:\n","[Dirty] Clean  Clean \n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 29, Agent Position: (0, 0), Performance: 27, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 30, Agent Position: (1, 0), Performance: 24, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 31, Agent Position: (0, 0), Performance: 23, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 32, Agent Position: (0, 1), Performance: 22, Environment state:\n"," Clean  Clean  Clean \n","[Clean] Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 33, Agent Position: (0, 0), Performance: 21, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 34, Agent Position: (1, 0), Performance: 19, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 35, Agent Position: (1, 1), Performance: 18, Environment state:\n"," Clean  Clean  Clean \n"," Clean [Clean]\n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 36, Agent Position: (1, 0), Performance: 17, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 37, Agent Position: (0, 0), Performance: 16, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 38, Agent Position: (0, 1), Performance: 15, Environment state:\n"," Clean  Clean  Clean \n","[Clean] Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 39, Agent Position: (1, 1), Performance: 14, Environment state:\n"," Clean  Clean  Clean \n"," Clean [Clean]\n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean \n","Step: 40, Agent Position: (1, 2), Performance: 13, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean \n"," Clean [Clean] Clean  Dirty \n"," Clean  Clean \n","Step: 41, Agent Position: (2, 2), Performance: 12, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean \n"," Clean  Clean [Clean] Dirty \n"," Clean  Clean \n","Step: 42, Agent Position: (1, 2), Performance: 11, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean \n"," Clean [Clean] Clean  Dirty \n"," Clean  Clean \n","Step: 43, Agent Position: (2, 2), Performance: 10, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean \n"," Clean  Clean [Clean] Dirty \n"," Clean  Clean \n","Step: 44, Agent Position: (1, 2), Performance: 9, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean \n"," Clean [Clean] Clean  Dirty \n"," Clean  Clean \n","Step: 45, Agent Position: (2, 2), Performance: 8, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean \n"," Clean  Clean [Clean] Dirty \n"," Clean  Clean \n","Step: 46, Agent Position: (3, 2), Performance: 7, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean \n"," Clean  Clean  Clean [Dirty]\n"," Clean  Clean \n","Step: 47, Agent Position: (3, 2), Performance: 17, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean \n"," Clean  Clean  Clean [Clean]\n"," Clean  Clean \n","Total steps: 48, Final performance: 17\n"]}],"source":["agent = SimpleReflexRandomAgent()\n","environment = VacuumCleanerWorld([\n","    [\"Dirty\", \"Dirty\", \"Clean\"],\n","    [\"Clean\", \"Dirty\"],\n","    [\"Dirty\", \"Clean\", \"Dirty\", \"Dirty\"],\n","    [\"Clean\", \"Dirty\"]\n","])\n","\n","simulation = EnvironmentSimulator(agent, (1, 1), environment)\n","\n","for step in range(total_steps):\n","    simulation.step(step)\n","\n","simulation.agent_performance -= sum(space == \"Dirty\" for row in environment.dirt_placement for space in row) * 10\n","print(f\"Total steps: {total_steps}, Final performance: {simulation.agent_performance}\")"]},{"cell_type":"markdown","metadata":{"id":"Yvz6HfeAf2Lh"},"source":["**Experiment 4:**\n","\n","Conversely, the randomized simple reflex agent successfully navigates the environment, and cleans all its spaces, achieving a final performance score of 17.\n","\n","---\n","\n","Let's explore a new environment configuration and modify the agent's initial positions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-d-mpxhLf2Lh","outputId":"ae93f1eb-7de6-42ab-8045-0f1a06503380"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial Agent Position: (1, 3), Performance: 0, Environment state:\n"," Dirty  Dirty  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty \n"," Dirty [Clean] Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 0, Agent Position: (2, 3), Performance: -1, Environment state:\n"," Dirty  Dirty  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean [Dirty] Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 1, Agent Position: (2, 3), Performance: 9, Environment state:\n"," Dirty  Dirty  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean [Clean] Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 2, Agent Position: (3, 3), Performance: 8, Environment state:\n"," Dirty  Dirty  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Clean [Dirty]\n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 3, Agent Position: (3, 3), Performance: 18, Environment state:\n"," Dirty  Dirty  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Clean [Clean]\n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 4, Agent Position: (2, 3), Performance: 15, Environment state:\n"," Dirty  Dirty  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 5, Agent Position: (1, 3), Performance: 14, Environment state:\n"," Dirty  Dirty  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty \n"," Dirty [Clean] Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 6, Agent Position: (0, 3), Performance: 13, Environment state:\n"," Dirty  Dirty  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty \n","[Dirty] Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 7, Agent Position: (0, 3), Performance: 23, Environment state:\n"," Dirty  Dirty  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty \n","[Clean] Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 8, Agent Position: (0, 2), Performance: 21, Environment state:\n"," Dirty  Dirty  Clean \n"," Dirty  Dirty  Dirty \n","[Dirty] Dirty  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 9, Agent Position: (0, 2), Performance: 31, Environment state:\n"," Dirty  Dirty  Clean \n"," Dirty  Dirty  Dirty \n","[Clean] Dirty  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 10, Agent Position: (0, 1), Performance: 30, Environment state:\n"," Dirty  Dirty  Clean \n","[Dirty] Dirty  Dirty \n"," Clean  Dirty  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 11, Agent Position: (0, 1), Performance: 40, Environment state:\n"," Dirty  Dirty  Clean \n","[Clean] Dirty  Dirty \n"," Clean  Dirty  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 12, Agent Position: (0, 0), Performance: 39, Environment state:\n","[Dirty] Dirty  Clean \n"," Clean  Dirty  Dirty \n"," Clean  Dirty  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 13, Agent Position: (0, 0), Performance: 49, Environment state:\n","[Clean] Dirty  Clean \n"," Clean  Dirty  Dirty \n"," Clean  Dirty  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 14, Agent Position: (1, 0), Performance: 47, Environment state:\n"," Clean [Dirty] Clean \n"," Clean  Dirty  Dirty \n"," Clean  Dirty  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 15, Agent Position: (1, 0), Performance: 57, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty  Dirty \n"," Clean  Dirty  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 16, Agent Position: (2, 0), Performance: 56, Environment state:\n"," Clean  Clean [Clean]\n"," Clean  Dirty  Dirty \n"," Clean  Dirty  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 17, Agent Position: (2, 1), Performance: 54, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty [Dirty]\n"," Clean  Dirty  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 18, Agent Position: (2, 1), Performance: 64, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty [Clean]\n"," Clean  Dirty  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 19, Agent Position: (2, 2), Performance: 63, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty  Clean \n"," Clean  Dirty [Dirty]\n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 20, Agent Position: (2, 2), Performance: 73, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty  Clean \n"," Clean  Dirty [Clean]\n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 21, Agent Position: (2, 3), Performance: 72, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty  Clean \n"," Clean  Dirty  Clean \n"," Clean  Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 22, Agent Position: (1, 3), Performance: 70, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty  Clean \n"," Clean  Dirty  Clean \n"," Clean [Clean] Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 23, Agent Position: (0, 3), Performance: 69, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty  Clean \n"," Clean  Dirty  Clean \n","[Clean] Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 24, Agent Position: (0, 2), Performance: 67, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty  Clean \n","[Clean] Dirty  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 25, Agent Position: (0, 1), Performance: 66, Environment state:\n"," Clean  Clean  Clean \n","[Clean] Dirty  Clean \n"," Clean  Dirty  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 26, Agent Position: (0, 0), Performance: 65, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Dirty  Clean \n"," Clean  Dirty  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 27, Agent Position: (1, 0), Performance: 63, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty  Clean \n"," Clean  Dirty  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 28, Agent Position: (2, 0), Performance: 62, Environment state:\n"," Clean  Clean [Clean]\n"," Clean  Dirty  Clean \n"," Clean  Dirty  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 29, Agent Position: (2, 1), Performance: 60, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty [Clean]\n"," Clean  Dirty  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 30, Agent Position: (2, 2), Performance: 59, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty  Clean \n"," Clean  Dirty [Clean]\n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 31, Agent Position: (2, 3), Performance: 58, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty  Clean \n"," Clean  Dirty  Clean \n"," Clean  Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 32, Agent Position: (1, 3), Performance: 56, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty  Clean \n"," Clean  Dirty  Clean \n"," Clean [Clean] Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 33, Agent Position: (0, 3), Performance: 55, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty  Clean \n"," Clean  Dirty  Clean \n","[Clean] Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 34, Agent Position: (0, 2), Performance: 53, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty  Clean \n","[Clean] Dirty  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 35, Agent Position: (0, 1), Performance: 52, Environment state:\n"," Clean  Clean  Clean \n","[Clean] Dirty  Clean \n"," Clean  Dirty  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 36, Agent Position: (0, 0), Performance: 51, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Dirty  Clean \n"," Clean  Dirty  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 37, Agent Position: (1, 0), Performance: 49, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty  Clean \n"," Clean  Dirty  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 38, Agent Position: (2, 0), Performance: 48, Environment state:\n"," Clean  Clean [Clean]\n"," Clean  Dirty  Clean \n"," Clean  Dirty  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 39, Agent Position: (2, 1), Performance: 46, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty [Clean]\n"," Clean  Dirty  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 40, Agent Position: (2, 2), Performance: 45, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty  Clean \n"," Clean  Dirty [Clean]\n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 41, Agent Position: (2, 3), Performance: 44, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty  Clean \n"," Clean  Dirty  Clean \n"," Clean  Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 42, Agent Position: (1, 3), Performance: 42, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty  Clean \n"," Clean  Dirty  Clean \n"," Clean [Clean] Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 43, Agent Position: (0, 3), Performance: 41, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty  Clean \n"," Clean  Dirty  Clean \n","[Clean] Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 44, Agent Position: (0, 2), Performance: 39, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty  Clean \n","[Clean] Dirty  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 45, Agent Position: (0, 1), Performance: 38, Environment state:\n"," Clean  Clean  Clean \n","[Clean] Dirty  Clean \n"," Clean  Dirty  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 46, Agent Position: (0, 0), Performance: 37, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Dirty  Clean \n"," Clean  Dirty  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 47, Agent Position: (1, 0), Performance: 35, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Dirty  Clean \n"," Clean  Dirty  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 48, Agent Position: (2, 0), Performance: 34, Environment state:\n"," Clean  Clean [Clean]\n"," Clean  Dirty  Clean \n"," Clean  Dirty  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 49, Agent Position: (2, 1), Performance: 32, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Dirty [Clean]\n"," Clean  Dirty  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Total steps: 50, Final performance: -18\n"]}],"source":["agent = SimpleReflexAgent()\n","environment = VacuumCleanerWorld([\n","    [\"Dirty\", \"Dirty\", \"Clean\"],\n","    [\"Dirty\", \"Dirty\", \"Dirty\"],\n","    [\"Dirty\", \"Dirty\", \"Dirty\"],\n","    [\"Dirty\", \"Clean\", \"Dirty\", \"Dirty\"],\n","    [\"Clean\", \"Dirty\"],\n","    [\"Dirty\"],\n","    [\"Clean\", \"Dirty\"]\n","])\n","\n","total_steps = 50\n","simulation = EnvironmentSimulator(agent, (1, 3), environment)\n","\n","for step in range(total_steps):\n","    simulation.step(step)\n","\n","simulation.agent_performance -= sum(space == \"Dirty\" for row in environment.dirt_placement for space in row) * 10\n","print(f\"Total steps: {total_steps}, Final performance: {simulation.agent_performance}\")"]},{"cell_type":"markdown","metadata":{"id":"HVNI48Rbf2Li"},"source":["**Experiment 5:**\n","\n","In this updated configuration, the simple reflex agent succeeds in covering a more extensive portion of the environment, resulting in an improved but still negative score of -18. This occurs because the agent continues to get trapped in loops, consequently failing to clean a substantial portion of the environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WG9yf5_sf2Li","outputId":"321c5ee0-3f37-42c4-9a94-9ca127f0a714"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial Agent Position: (2, 1), Performance: 0, Environment state:\n"," Dirty  Dirty  Clean \n"," Dirty  Dirty [Dirty]\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 0, Agent Position: (2, 1), Performance: 10, Environment state:\n"," Dirty  Dirty  Clean \n"," Dirty  Dirty [Clean]\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 1, Agent Position: (1, 1), Performance: 9, Environment state:\n"," Dirty  Dirty  Clean \n"," Dirty [Dirty] Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 2, Agent Position: (1, 1), Performance: 19, Environment state:\n"," Dirty  Dirty  Clean \n"," Dirty [Clean] Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 3, Agent Position: (0, 1), Performance: 18, Environment state:\n"," Dirty  Dirty  Clean \n","[Dirty] Clean  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 4, Agent Position: (0, 1), Performance: 28, Environment state:\n"," Dirty  Dirty  Clean \n","[Clean] Clean  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 5, Agent Position: (0, 0), Performance: 26, Environment state:\n","[Dirty] Dirty  Clean \n"," Clean  Clean  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 6, Agent Position: (0, 0), Performance: 36, Environment state:\n","[Clean] Dirty  Clean \n"," Clean  Clean  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 7, Agent Position: (0, 1), Performance: 35, Environment state:\n"," Clean  Dirty  Clean \n","[Clean] Clean  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 8, Agent Position: (0, 0), Performance: 33, Environment state:\n","[Clean] Dirty  Clean \n"," Clean  Clean  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 9, Agent Position: (1, 0), Performance: 30, Environment state:\n"," Clean [Dirty] Clean \n"," Clean  Clean  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 10, Agent Position: (1, 0), Performance: 40, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Clean  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 11, Agent Position: (0, 0), Performance: 39, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Clean  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 12, Agent Position: (1, 0), Performance: 38, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Clean  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 13, Agent Position: (1, 1), Performance: 37, Environment state:\n"," Clean  Clean  Clean \n"," Clean [Clean] Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 14, Agent Position: (0, 1), Performance: 36, Environment state:\n"," Clean  Clean  Clean \n","[Clean] Clean  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 15, Agent Position: (0, 0), Performance: 34, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Clean  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 16, Agent Position: (1, 0), Performance: 31, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Clean  Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 17, Agent Position: (1, 1), Performance: 30, Environment state:\n"," Clean  Clean  Clean \n"," Clean [Clean] Clean \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 18, Agent Position: (2, 1), Performance: 29, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean [Clean]\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 19, Agent Position: (2, 2), Performance: 27, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Dirty  Dirty [Dirty]\n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 20, Agent Position: (2, 2), Performance: 37, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Dirty  Dirty [Clean]\n"," Dirty  Clean  Dirty  Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 21, Agent Position: (2, 3), Performance: 36, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Dirty  Dirty  Clean \n"," Dirty  Clean [Dirty] Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 22, Agent Position: (2, 3), Performance: 46, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Dirty  Dirty  Clean \n"," Dirty  Clean [Clean] Dirty \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 23, Agent Position: (3, 3), Performance: 45, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Dirty  Dirty  Clean \n"," Dirty  Clean  Clean [Dirty]\n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 24, Agent Position: (3, 3), Performance: 55, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Dirty  Dirty  Clean \n"," Dirty  Clean  Clean [Clean]\n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 25, Agent Position: (2, 3), Performance: 53, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Dirty  Dirty  Clean \n"," Dirty  Clean [Clean] Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 26, Agent Position: (1, 3), Performance: 52, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Dirty  Dirty  Clean \n"," Dirty [Clean] Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 27, Agent Position: (1, 2), Performance: 51, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Dirty [Dirty] Clean \n"," Dirty  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 28, Agent Position: (1, 2), Performance: 61, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Dirty [Clean] Clean \n"," Dirty  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 29, Agent Position: (0, 2), Performance: 60, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","[Dirty] Clean  Clean \n"," Dirty  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 30, Agent Position: (0, 2), Performance: 70, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","[Clean] Clean  Clean \n"," Dirty  Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 31, Agent Position: (0, 3), Performance: 69, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","[Dirty] Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 32, Agent Position: (0, 3), Performance: 79, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","[Clean] Clean  Clean  Clean \n"," Clean  Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 33, Agent Position: (0, 4), Performance: 78, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","[Clean] Dirty \n"," Dirty \n"," Clean  Dirty \n","Step: 34, Agent Position: (1, 4), Performance: 77, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean [Dirty]\n"," Dirty \n"," Clean  Dirty \n","Step: 35, Agent Position: (1, 4), Performance: 87, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean [Clean]\n"," Dirty \n"," Clean  Dirty \n","Step: 36, Agent Position: (0, 4), Performance: 85, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","[Clean] Clean \n"," Dirty \n"," Clean  Dirty \n","Step: 37, Agent Position: (1, 4), Performance: 84, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean [Clean]\n"," Dirty \n"," Clean  Dirty \n","Step: 38, Agent Position: (0, 4), Performance: 83, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","[Clean] Clean \n"," Dirty \n"," Clean  Dirty \n","Step: 39, Agent Position: (1, 4), Performance: 82, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean [Clean]\n"," Dirty \n"," Clean  Dirty \n","Step: 40, Agent Position: (0, 4), Performance: 80, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","[Clean] Clean \n"," Dirty \n"," Clean  Dirty \n","Step: 41, Agent Position: (0, 3), Performance: 78, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","[Clean] Clean  Clean  Clean \n"," Clean  Clean \n"," Dirty \n"," Clean  Dirty \n","Step: 42, Agent Position: (0, 4), Performance: 77, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","[Clean] Clean \n"," Dirty \n"," Clean  Dirty \n","Step: 43, Agent Position: (0, 5), Performance: 76, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean \n","[Dirty]\n"," Clean  Dirty \n","Step: 44, Agent Position: (0, 5), Performance: 86, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean \n","[Clean]\n"," Clean  Dirty \n","Step: 45, Agent Position: (0, 4), Performance: 84, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","[Clean] Clean \n"," Clean \n"," Clean  Dirty \n","Step: 46, Agent Position: (0, 3), Performance: 82, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","[Clean] Clean  Clean  Clean \n"," Clean  Clean \n"," Clean \n"," Clean  Dirty \n","Step: 47, Agent Position: (0, 2), Performance: 81, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","[Clean] Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean \n"," Clean \n"," Clean  Dirty \n","Total steps: 48, Final performance: 71\n"]}],"source":["agent = SimpleReflexRandomAgent()\n","environment = VacuumCleanerWorld([\n","    [\"Dirty\", \"Dirty\", \"Clean\"],\n","    [\"Dirty\", \"Dirty\", \"Dirty\"],\n","    [\"Dirty\", \"Dirty\", \"Dirty\"],\n","    [\"Dirty\", \"Clean\", \"Dirty\", \"Dirty\"],\n","    [\"Clean\", \"Dirty\"],\n","    [\"Dirty\"],\n","    [\"Clean\", \"Dirty\"]\n","])\n","\n","simulation = EnvironmentSimulator(agent, (2, 1), environment)\n","\n","for step in range(total_steps):\n","    simulation.step(step)\n","\n","simulation.agent_performance -= sum(space == \"Dirty\" for row in environment.dirt_placement for space in row) * 10\n","print(f\"Total steps: {total_steps}, Final performance: {simulation.agent_performance}\")"]},{"cell_type":"markdown","metadata":{"id":"DhhZwsXBf2Li"},"source":["**Experiment 6:**\n","\n","On the other hand, the randomized simple reflex agent continues to demonstrate its capability to effectively navigate the environment and clean all its spaces, culminating in an impressive final performance score of 71.\n","\n","---\n","\n","3. While the randomized agent excels in exploration, its movements are irrational, relying solely on chance to cover all spaces effectively. This implies that its performance is contingent on having a small environment and ample time to accidentally encounter every area. The following code is designed to put this specific scenario to the test, evaluating how well the agent performs under constrained time conditions and a large environment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y7hlZcCdf2Li","outputId":"32090caa-1273-471f-c690-cd275f1dd742"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial Agent Position: (1, 2), Performance: 0, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean [Dirty] Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 0, Agent Position: (1, 2), Performance: 10, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean [Clean] Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 1, Agent Position: (0, 2), Performance: 9, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n","[Clean] Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 2, Agent Position: (0, 1), Performance: 7, Environment state:\n"," Dirty  Dirty  Dirty \n","[Dirty] Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 3, Agent Position: (0, 1), Performance: 17, Environment state:\n"," Dirty  Dirty  Dirty \n","[Clean] Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 4, Agent Position: (0, 0), Performance: 16, Environment state:\n","[Dirty] Dirty  Dirty \n"," Clean  Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 5, Agent Position: (0, 0), Performance: 26, Environment state:\n","[Clean] Dirty  Dirty \n"," Clean  Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 6, Agent Position: (1, 0), Performance: 24, Environment state:\n"," Clean [Dirty] Dirty \n"," Clean  Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 7, Agent Position: (1, 0), Performance: 34, Environment state:\n"," Clean [Clean] Dirty \n"," Clean  Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 8, Agent Position: (1, 1), Performance: 33, Environment state:\n"," Clean  Clean  Dirty \n"," Clean [Clean] Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 9, Agent Position: (1, 0), Performance: 32, Environment state:\n"," Clean [Clean] Dirty \n"," Clean  Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 10, Agent Position: (2, 0), Performance: 30, Environment state:\n"," Clean  Clean [Dirty]\n"," Clean  Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 11, Agent Position: (2, 0), Performance: 40, Environment state:\n"," Clean  Clean [Clean]\n"," Clean  Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 12, Agent Position: (1, 0), Performance: 39, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 13, Agent Position: (1, 1), Performance: 38, Environment state:\n"," Clean  Clean  Clean \n"," Clean [Clean] Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 14, Agent Position: (1, 0), Performance: 37, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 15, Agent Position: (0, 0), Performance: 36, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 16, Agent Position: (1, 0), Performance: 33, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 17, Agent Position: (1, 1), Performance: 32, Environment state:\n"," Clean  Clean  Clean \n"," Clean [Clean] Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 18, Agent Position: (1, 0), Performance: 31, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 19, Agent Position: (2, 0), Performance: 30, Environment state:\n"," Clean  Clean [Clean]\n"," Clean  Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 20, Agent Position: (1, 0), Performance: 29, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 21, Agent Position: (1, 1), Performance: 28, Environment state:\n"," Clean  Clean  Clean \n"," Clean [Clean] Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 22, Agent Position: (1, 2), Performance: 27, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Dirty \n"," Clean [Clean] Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 23, Agent Position: (0, 2), Performance: 26, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean  Dirty \n","[Clean] Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 24, Agent Position: (0, 1), Performance: 25, Environment state:\n"," Clean  Clean  Clean \n","[Clean] Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 25, Agent Position: (0, 0), Performance: 23, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 26, Agent Position: (1, 0), Performance: 21, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 27, Agent Position: (0, 0), Performance: 20, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 28, Agent Position: (1, 0), Performance: 17, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 29, Agent Position: (1, 1), Performance: 16, Environment state:\n"," Clean  Clean  Clean \n"," Clean [Clean] Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 30, Agent Position: (0, 1), Performance: 15, Environment state:\n"," Clean  Clean  Clean \n","[Clean] Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 31, Agent Position: (1, 1), Performance: 14, Environment state:\n"," Clean  Clean  Clean \n"," Clean [Clean] Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 32, Agent Position: (1, 0), Performance: 13, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Clean  Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 33, Agent Position: (1, 1), Performance: 12, Environment state:\n"," Clean  Clean  Clean \n"," Clean [Clean] Dirty \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 34, Agent Position: (2, 1), Performance: 11, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean [Dirty]\n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 35, Agent Position: (2, 1), Performance: 21, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean [Clean]\n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 36, Agent Position: (2, 0), Performance: 20, Environment state:\n"," Clean  Clean [Clean]\n"," Clean  Clean  Clean \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 37, Agent Position: (1, 0), Performance: 19, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 38, Agent Position: (0, 0), Performance: 18, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 39, Agent Position: (0, 1), Performance: 17, Environment state:\n"," Clean  Clean  Clean \n","[Clean] Clean  Clean \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 40, Agent Position: (0, 0), Performance: 16, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 41, Agent Position: (0, 1), Performance: 15, Environment state:\n"," Clean  Clean  Clean \n","[Clean] Clean  Clean \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 42, Agent Position: (0, 0), Performance: 13, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 43, Agent Position: (1, 0), Performance: 11, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 44, Agent Position: (1, 1), Performance: 10, Environment state:\n"," Clean  Clean  Clean \n"," Clean [Clean] Clean \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 45, Agent Position: (1, 0), Performance: 9, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 46, Agent Position: (0, 0), Performance: 8, Environment state:\n","[Clean] Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 47, Agent Position: (1, 0), Performance: 7, Environment state:\n"," Clean [Clean] Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Total steps: 48, Final performance: -63\n"]}],"source":["agent = SimpleReflexRandomAgent()\n","environment = VacuumCleanerWorld([\n","    [\"Dirty\", \"Dirty\", \"Dirty\"],\n","    [\"Dirty\", \"Clean\", \"Dirty\"],\n","    [\"Clean\", \"Dirty\", \"Dirty\"],\n","    [\"Dirty\", \"Clean\", \"Dirty\"],\n","    [\"Dirty\", \"Clean\", \"Clean\"],\n","    [\"Dirty\", \"Dirty\", \"Dirty\"]\n","])\n","\n","simulation = EnvironmentSimulator(agent, (1, 2), environment)\n","\n","for step in range(total_steps):\n","    simulation.step(step)\n","\n","simulation.agent_performance -= sum(space == \"Dirty\" for row in environment.dirt_placement for space in row) * 10\n","print(f\"Total steps: {total_steps}, Final performance: {simulation.agent_performance}\")"]},{"cell_type":"markdown","metadata":{"id":"uMqNOCLtf2Li"},"source":["**Experiment 7:**\n","\n","As hypothesized, for this scenario, the randomized simple reflex agent is unable to clean all areas of the environment, moving erratically without achieving any meaningful progress, leading to a final score of -63."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JKPYUgs4f2Li","outputId":"95ddc375-f594-444e-a036-0dd60a9ee8dd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial Agent Position: (1, 2), Performance: 0, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean [Dirty] Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 0, Agent Position: (1, 2), Performance: 10, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean [Clean] Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 1, Agent Position: (2, 2), Performance: 9, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean  Clean [Dirty]\n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 2, Agent Position: (2, 2), Performance: 19, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean  Clean [Clean]\n"," Dirty  Clean  Dirty \n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 3, Agent Position: (2, 3), Performance: 17, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean  Clean  Clean \n"," Dirty  Clean [Dirty]\n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 4, Agent Position: (2, 3), Performance: 27, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean  Clean  Clean \n"," Dirty  Clean [Clean]\n"," Dirty  Clean  Clean \n"," Dirty  Dirty  Dirty \n","Step: 5, Agent Position: (2, 4), Performance: 26, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean  Clean  Clean \n"," Dirty  Clean  Clean \n"," Dirty  Clean [Clean]\n"," Dirty  Dirty  Dirty \n","Step: 6, Agent Position: (2, 5), Performance: 25, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean  Clean  Clean \n"," Dirty  Clean  Clean \n"," Dirty  Clean  Clean \n"," Dirty  Dirty [Dirty]\n","Step: 7, Agent Position: (2, 5), Performance: 35, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean  Clean  Clean \n"," Dirty  Clean  Clean \n"," Dirty  Clean  Clean \n"," Dirty  Dirty [Clean]\n","Step: 8, Agent Position: (1, 5), Performance: 33, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean  Clean  Clean \n"," Dirty  Clean  Clean \n"," Dirty  Clean  Clean \n"," Dirty [Dirty] Clean \n","Step: 9, Agent Position: (1, 5), Performance: 43, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean  Clean  Clean \n"," Dirty  Clean  Clean \n"," Dirty  Clean  Clean \n"," Dirty [Clean] Clean \n","Step: 10, Agent Position: (0, 5), Performance: 42, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean  Clean  Clean \n"," Dirty  Clean  Clean \n"," Dirty  Clean  Clean \n","[Dirty] Clean  Clean \n","Step: 11, Agent Position: (0, 5), Performance: 52, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean  Clean  Clean \n"," Dirty  Clean  Clean \n"," Dirty  Clean  Clean \n","[Clean] Clean  Clean \n","Step: 12, Agent Position: (0, 4), Performance: 50, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean  Clean  Clean \n"," Dirty  Clean  Clean \n","[Dirty] Clean  Clean \n"," Clean  Clean  Clean \n","Step: 13, Agent Position: (0, 4), Performance: 60, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean  Clean  Clean \n"," Dirty  Clean  Clean \n","[Clean] Clean  Clean \n"," Clean  Clean  Clean \n","Step: 14, Agent Position: (0, 3), Performance: 59, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean  Clean  Clean \n","[Dirty] Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","Step: 15, Agent Position: (0, 3), Performance: 69, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean  Clean  Clean \n","[Clean] Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","Step: 16, Agent Position: (0, 2), Performance: 68, Environment state:\n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n","[Clean] Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","Step: 17, Agent Position: (0, 1), Performance: 67, Environment state:\n"," Dirty  Dirty  Dirty \n","[Dirty] Clean  Dirty \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","Step: 18, Agent Position: (0, 1), Performance: 77, Environment state:\n"," Dirty  Dirty  Dirty \n","[Clean] Clean  Dirty \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","Step: 19, Agent Position: (0, 0), Performance: 76, Environment state:\n","[Dirty] Dirty  Dirty \n"," Clean  Clean  Dirty \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","Step: 20, Agent Position: (0, 0), Performance: 86, Environment state:\n","[Clean] Dirty  Dirty \n"," Clean  Clean  Dirty \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","Step: 21, Agent Position: (1, 0), Performance: 84, Environment state:\n"," Clean [Dirty] Dirty \n"," Clean  Clean  Dirty \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","Step: 22, Agent Position: (1, 0), Performance: 94, Environment state:\n"," Clean [Clean] Dirty \n"," Clean  Clean  Dirty \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","Step: 23, Agent Position: (2, 0), Performance: 93, Environment state:\n"," Clean  Clean [Dirty]\n"," Clean  Clean  Dirty \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","Step: 24, Agent Position: (2, 0), Performance: 103, Environment state:\n"," Clean  Clean [Clean]\n"," Clean  Clean  Dirty \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","Step: 25, Agent Position: (2, 1), Performance: 101, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean [Dirty]\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","Step: 26, Agent Position: (2, 1), Performance: 111, Environment state:\n"," Clean  Clean  Clean \n"," Clean  Clean [Clean]\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n","Total steps: 27, Final performance: 111\n"]}],"source":["agent = SimpleReflexAgent()\n","environment = VacuumCleanerWorld([\n","    [\"Dirty\", \"Dirty\", \"Dirty\"],\n","    [\"Dirty\", \"Clean\", \"Dirty\"],\n","    [\"Clean\", \"Dirty\", \"Dirty\"],\n","    [\"Dirty\", \"Clean\", \"Dirty\"],\n","    [\"Dirty\", \"Clean\", \"Clean\"],\n","    [\"Dirty\", \"Dirty\", \"Dirty\"]\n","])\n","\n","simulation = EnvironmentSimulator(agent, (1, 2), environment)\n","\n","total_steps = 27\n","for step in range(total_steps):\n","    simulation.step(step)\n","\n","simulation.agent_performance -= sum(space == \"Dirty\" for row in environment.dirt_placement for space in row) * 10\n","print(f\"Total steps: {total_steps}, Final performance: {simulation.agent_performance}\")"]},{"cell_type":"markdown","metadata":{"id":"lcaefTcAf2Li"},"source":["**Experiment 8:**\n","\n","Conversely, for this environment configuration, the simple reflex agent effectively cleans every space in the environment, securing an impressive final score of 111.\n","\n","---\n","\n","4. A reflex agent with state, commonly referred to as a model-based agent [1], can significantly surpass the performance of a simple reflex agent. Its underlying advantage lies in its capacity to monitor and dynamically update the environment's current state in response to its actions. This functionality allows the agent to determine its approximate current location, catalogue the spaces it has visited, and determinine its next action. This type of agent is inherently rational, as it utilizes its predefined rules and internal state to choose actions expected to optimize its performance metrics. Below, the provided code implements a model-based agent and evaluates its performance across multiple environment configurations."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"rUtUX3IHf2Li","executionInfo":{"status":"ok","timestamp":1709707614664,"user_tz":240,"elapsed":202,"user":{"displayName":"Alex Demel-Pacheco","userId":"13479886468262690947"}}},"outputs":[],"source":["class ModelBasedReflexAgent:\n","    def __init__(self):\n","        self.state = [['?', '?', '?'], ['?', '*', '?'], ['?', '?', '?']]\n","        self.rules = {\n","            \"Dirty\": \"Suck\",\n","            \"Clean\": \"Move\",\n","            \"Left-Boundary\": \"Right\",\n","            \"Right-Boundary\": \"Left\",\n","            \"Top-Boundary\": \"Down\",\n","            \"Bottom-Boundary\": \"Up\",\n","        }\n","        self.last_action = None\n","\n","    def update_state(self, percept):\n","        x, y = 0, 0\n","        for agent_y, row in enumerate(self.state):\n","            for agent_x, col in enumerate(row):\n","                if col == '*':\n","                    x, y = agent_x, agent_y\n","\n","        if percept in [\"Dirty\", \"Clean\"] or self.last_action == \"Suck\": self.state[y][x] = 'o'\n","        if \"Boundary\" in percept:\n","            if percept == \"Left-Boundary\" and x > 0 and self.state[y][x-1] != '?': [row.insert(0, '?') for row in self.state]; x += 1\n","            elif percept == \"Right-Boundary\" and x < len(self.state[0]) - 1 and self.state[y][x+1] != '?': [row.append('?') for row in self.state]\n","            elif percept == \"Top-Boundary\" and y > 0 and self.state[y-1][x] != '?': self.state.insert(0, ['?'] * len(self.state[0])); y += 1\n","            elif percept == \"Bottom-Boundary\" and y < len(self.state) - 1 and self.state[y+1][x] != '?': self.state.append(['?'] * len(self.state[0]))\n","\n","            topCheck = (self.last_action == 'Up' and percept == \"Top-Boundary\")\n","            bottomCheck = (self.last_action == 'Down' and percept == \"Bottom-Boundary\")\n","\n","            if self.last_action != None and self.last_action in percept or topCheck or bottomCheck:\n","                self.state[y][x] = 'X'\n","                if self.last_action == \"Left\": x += 2\n","                elif self.last_action == \"Right\": x -= 2\n","                elif self.last_action == \"Up\": y += 2\n","                elif self.last_action == \"Down\": y -= 2\n","                if self.state[y][x] == 'X':\n","                    if self.last_action == \"Left\":\n","                        dx, dy = (-1, 1)\n","                    elif self.last_action == \"Right\":\n","                        dx, dy = (1, -1)\n","                    elif self.last_action == \"Up\":\n","                        dx, dy = (1, -1)\n","                    elif self.last_action == \"Down\":\n","                        dx, dy = (-1, 1)\n","                    x, y = x + dx, y + dy\n","                    percept = \"Change Direction\"\n","            else:\n","                if self.last_action == \"Left\": x += 1\n","                elif self.last_action == \"Right\": x -= 1\n","                elif self.last_action == \"Up\": y += 1\n","                elif self.last_action == \"Down\": y -= 1\n","                if percept == \"Left-Boundary\": x += 1\n","                elif percept == \"Right-Boundary\": x -= 1\n","                elif percept == \"Top-Boundary\": y += 1\n","                elif percept == \"Bottom-Boundary\": y -= 1\n","\n","        if percept == \"Clean\":\n","            unknown_directions = [(\"Left\", x > 0 and self.state[y][x-1] == '?'),\n","                (\"Right\", x < len(self.state[0]) - 1 and self.state[y][x+1] == '?'),\n","                (\"Up\", y > 0 and self.state[y-1][x] == '?'),\n","                (\"Down\", y < len(self.state) - 1 and self.state[y+1][x] == '?')]\n","            direction = next((dir for dir, cond in unknown_directions if cond), None)\n","            if not direction:\n","                directions, positions = [(-1, 0), (1, 0), (0, -1), (0, 1)], []\n","                for i, row in enumerate(self.state):\n","                    for j, cell in enumerate(row):\n","                        if cell == 'o' and any(self.state[i + dx][j + dy] == '?' for dx, dy in directions):\n","                            positions.append((i, j))\n","                nearest = None\n","                min_distance = float('inf')\n","                for position in positions:\n","                    distance = ((position[0] - x) ** 2 + (position[1] - y) ** 2) ** 0.5\n","                    if distance < min_distance:\n","                        min_distance = distance\n","                        nearest = position\n","\n","                if nearest[0] - x > 0: direction = \"Right\"\n","                elif nearest[0] - x < 0: direction = \"Left\"\n","                elif nearest[1] - y > 0: direction = \"Up\"\n","                else: direction = \"Down\"\n","\n","            x += {\"Left\": -1, \"Right\": 1}.get(direction, 0)\n","            y += {\"Up\": -1, \"Down\": 1}.get(direction, 0)\n","\n","        if y + 1 > len(self.state) - 1: self.state.append(['?'] * len(self.state[0]))\n","        elif y - 1 < 0: self.state.insert(0, ['?'] * len(self.state[0])); y += 1\n","        if x + 1 > len(self.state[0]) - 1: [row.append('?') for row in self.state]\n","        elif x - 1 < 0: [row.insert(0, '?') for row in self.state]; x += 1\n","        self.state[y][x] = '*'\n","\n","        if percept == 'Clean':\n","            return direction\n","        return percept\n","\n","    def rule_match(self, percept):\n","        return self.rules[percept]\n","\n","    def action(self, percept):\n","        direction = self.update_state(percept)\n","        action = self.rule_match(percept)\n","\n","        if action == \"Move\":\n","            action = direction\n","        elif direction == \"Change Direction\":\n","            action_map = {\n","                \"Left\": \"Down\",\n","                \"Right\": \"Up\",\n","                \"Up\": \"Right\",\n","                \"Down\": \"Left\"\n","            }\n","            action = action_map.get(self.last_action, action)\n","\n","        self.last_action = action\n","\n","        return action"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cKmGl-_ef2Li","executionInfo":{"status":"ok","timestamp":1709707654205,"user_tz":240,"elapsed":187,"user":{"displayName":"Alex Demel-Pacheco","userId":"13479886468262690947"}},"outputId":"f54a5ee3-907b-407a-cc88-83142d7490b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Initial Agent Position: (4, 3), Performance: 0, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 0, Agent Position: (3, 3), Performance: -1, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty [Dirty]\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 1, Agent Position: (3, 3), Performance: 9, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty [Clean]\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 2, Agent Position: (2, 3), Performance: 7, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty [Dirty] Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 3, Agent Position: (2, 3), Performance: 17, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty [Clean] Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 4, Agent Position: (1, 3), Performance: 16, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty [Dirty] Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 5, Agent Position: (1, 3), Performance: 26, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty [Clean] Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 6, Agent Position: (0, 3), Performance: 25, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","[Dirty] Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 7, Agent Position: (0, 3), Performance: 35, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","[Clean] Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 8, Agent Position: (1, 3), Performance: 33, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Clean [Clean] Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 9, Agent Position: (1, 2), Performance: 32, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty [Dirty] Dirty  Dirty \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 10, Agent Position: (1, 2), Performance: 42, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty [Clean] Dirty  Dirty \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 11, Agent Position: (0, 2), Performance: 41, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","[Dirty] Clean  Dirty  Dirty \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 12, Agent Position: (0, 2), Performance: 51, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","[Clean] Clean  Dirty  Dirty \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 13, Agent Position: (1, 2), Performance: 49, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Clean [Clean] Dirty  Dirty \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 14, Agent Position: (2, 2), Performance: 48, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Clean  Clean [Dirty] Dirty \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 15, Agent Position: (2, 2), Performance: 58, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Clean  Clean [Clean] Dirty \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 16, Agent Position: (3, 2), Performance: 57, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Clean  Clean  Clean [Dirty]\n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 17, Agent Position: (3, 2), Performance: 67, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Clean  Clean  Clean [Clean]\n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 18, Agent Position: (2, 2), Performance: 65, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Clean  Clean [Clean] Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 19, Agent Position: (2, 1), Performance: 64, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty [Dirty] Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 20, Agent Position: (2, 1), Performance: 74, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty [Clean] Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 21, Agent Position: (1, 1), Performance: 73, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty [Dirty] Clean  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 22, Agent Position: (1, 1), Performance: 83, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty [Clean] Clean  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 23, Agent Position: (0, 1), Performance: 82, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n","[Dirty] Clean  Clean  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 24, Agent Position: (0, 1), Performance: 92, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n","[Clean] Clean  Clean  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 25, Agent Position: (1, 1), Performance: 90, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Clean [Clean] Clean  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 26, Agent Position: (1, 0), Performance: 89, Environment state:\n"," Dirty [Dirty] Dirty  Dirty \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 27, Agent Position: (1, 0), Performance: 99, Environment state:\n"," Dirty [Clean] Dirty  Dirty \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 28, Agent Position: (0, 0), Performance: 98, Environment state:\n","[Dirty] Clean  Dirty  Dirty \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 29, Agent Position: (0, 0), Performance: 108, Environment state:\n","[Clean] Clean  Dirty  Dirty \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 30, Agent Position: (1, 0), Performance: 106, Environment state:\n"," Clean [Clean] Dirty  Dirty \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 31, Agent Position: (2, 0), Performance: 105, Environment state:\n"," Clean  Clean [Dirty] Dirty \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 32, Agent Position: (2, 0), Performance: 115, Environment state:\n"," Clean  Clean [Clean] Dirty \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 33, Agent Position: (3, 0), Performance: 114, Environment state:\n"," Clean  Clean  Clean [Dirty]\n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 34, Agent Position: (3, 0), Performance: 124, Environment state:\n"," Clean  Clean  Clean [Clean]\n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 35, Agent Position: (2, 0), Performance: 122, Environment state:\n"," Clean  Clean [Clean] Clean \n"," Clean  Clean  Clean  Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 36, Agent Position: (2, 1), Performance: 120, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean [Clean] Dirty \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 37, Agent Position: (3, 1), Performance: 119, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean [Dirty]\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 38, Agent Position: (3, 1), Performance: 129, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean [Clean]\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 39, Agent Position: (2, 1), Performance: 127, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean [Clean] Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 40, Agent Position: (3, 1), Performance: 126, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean [Clean]\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 41, Agent Position: (3, 2), Performance: 125, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean [Clean]\n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 42, Agent Position: (3, 3), Performance: 124, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean [Clean]\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 43, Agent Position: (3, 4), Performance: 123, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty [Dirty]\n"," Dirty  Dirty  Dirty  Dirty \n","Step: 44, Agent Position: (3, 4), Performance: 133, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty  Dirty [Clean]\n"," Dirty  Dirty  Dirty  Dirty \n","Step: 45, Agent Position: (2, 4), Performance: 132, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty [Dirty] Clean \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 46, Agent Position: (2, 4), Performance: 142, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty  Dirty [Clean] Clean \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 47, Agent Position: (1, 4), Performance: 141, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty [Dirty] Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 48, Agent Position: (1, 4), Performance: 151, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty [Clean] Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 49, Agent Position: (0, 4), Performance: 150, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","[Dirty] Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 50, Agent Position: (0, 4), Performance: 160, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","[Clean] Clean  Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 51, Agent Position: (1, 4), Performance: 158, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean [Clean] Clean  Clean \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 52, Agent Position: (1, 5), Performance: 157, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty [Dirty] Dirty  Dirty \n","Step: 53, Agent Position: (1, 5), Performance: 167, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Dirty [Clean] Dirty  Dirty \n","Step: 54, Agent Position: (0, 5), Performance: 166, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","[Dirty] Clean  Dirty  Dirty \n","Step: 55, Agent Position: (0, 5), Performance: 176, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","[Clean] Clean  Dirty  Dirty \n","Step: 56, Agent Position: (1, 5), Performance: 174, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean [Clean] Dirty  Dirty \n","Step: 57, Agent Position: (2, 5), Performance: 173, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean [Dirty] Dirty \n","Step: 58, Agent Position: (2, 5), Performance: 183, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean [Clean] Dirty \n","Step: 59, Agent Position: (3, 5), Performance: 182, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean [Dirty]\n","Step: 60, Agent Position: (3, 5), Performance: 192, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n"," Clean  Clean  Clean [Clean]\n","Total steps: 61, Final performance: 192\n"]}],"source":["agent = ModelBasedReflexAgent()\n","environment = VacuumCleanerWorld([\n","    [\"Dirty\", \"Dirty\", \"Dirty\", \"Dirty\"],\n","    [\"Dirty\", \"Dirty\", \"Dirty\", \"Dirty\"],\n","    [\"Dirty\", \"Dirty\", \"Dirty\", \"Dirty\"],\n","    [\"Dirty\", \"Dirty\", \"Dirty\", \"Dirty\"],\n","    [\"Dirty\", \"Dirty\", \"Dirty\", \"Dirty\"],\n","    [\"Dirty\", \"Dirty\", \"Dirty\", \"Dirty\"]\n","])\n","\n","simulation = EnvironmentSimulator(agent, (4, 3), environment)\n","\n","total_steps = 61\n","for step in range(total_steps):\n","    simulation.step(step)\n","\n","simulation.agent_performance -= sum(space == \"Dirty\" for row in environment.dirt_placement for space in row) * 10\n","print(f\"Total steps: {total_steps}, Final performance: {simulation.agent_performance}\")"]},{"cell_type":"markdown","metadata":{"id":"RlloqVcyf2Lj"},"source":["**Experiment 9:**\n","\n","As evidenced by the simulation, the model-based agent excels in navigating large environments and meticulously cleaning every space, showcasing its exploratory capabilities and culminating in an outstanding performance score of 192."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"McqkRMSkf2Lj","outputId":"cb4fb662-3131-4266-9ec6-cdb912ee8ae6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial Agent Position: (0, 0), Performance: 0, Environment state:\n","[Dirty] Dirty  Dirty  Dirty \n"," Dirty \n"," Dirty  Clean  Dirty \n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 0, Agent Position: (0, 0), Performance: 10, Environment state:\n","[Clean] Dirty  Dirty  Dirty \n"," Dirty \n"," Dirty  Clean  Dirty \n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 1, Agent Position: (1, 0), Performance: 8, Environment state:\n"," Clean [Dirty] Dirty  Dirty \n"," Dirty \n"," Dirty  Clean  Dirty \n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 2, Agent Position: (1, 0), Performance: 18, Environment state:\n"," Clean [Clean] Dirty  Dirty \n"," Dirty \n"," Dirty  Clean  Dirty \n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 3, Agent Position: (2, 0), Performance: 17, Environment state:\n"," Clean  Clean [Dirty] Dirty \n"," Dirty \n"," Dirty  Clean  Dirty \n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 4, Agent Position: (2, 0), Performance: 27, Environment state:\n"," Clean  Clean [Clean] Dirty \n"," Dirty \n"," Dirty  Clean  Dirty \n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 5, Agent Position: (3, 0), Performance: 26, Environment state:\n"," Clean  Clean  Clean [Dirty]\n"," Dirty \n"," Dirty  Clean  Dirty \n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 6, Agent Position: (3, 0), Performance: 36, Environment state:\n"," Clean  Clean  Clean [Clean]\n"," Dirty \n"," Dirty  Clean  Dirty \n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 7, Agent Position: (2, 0), Performance: 34, Environment state:\n"," Clean  Clean [Clean] Clean \n"," Dirty \n"," Dirty  Clean  Dirty \n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 8, Agent Position: (1, 0), Performance: 31, Environment state:\n"," Clean [Clean] Clean  Clean \n"," Dirty \n"," Dirty  Clean  Dirty \n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 9, Agent Position: (0, 0), Performance: 28, Environment state:\n","[Clean] Clean  Clean  Clean \n"," Dirty \n"," Dirty  Clean  Dirty \n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 10, Agent Position: (0, 1), Performance: 26, Environment state:\n"," Clean  Clean  Clean  Clean \n","[Dirty]\n"," Dirty  Clean  Dirty \n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 11, Agent Position: (0, 1), Performance: 36, Environment state:\n"," Clean  Clean  Clean  Clean \n","[Clean]\n"," Dirty  Clean  Dirty \n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 12, Agent Position: (0, 2), Performance: 34, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n","[Dirty] Clean  Dirty \n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 13, Agent Position: (0, 2), Performance: 44, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n","[Clean] Clean  Dirty \n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 14, Agent Position: (1, 2), Performance: 42, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean [Clean] Dirty \n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 15, Agent Position: (2, 2), Performance: 41, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean [Dirty]\n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 16, Agent Position: (2, 2), Performance: 51, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean [Clean]\n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 17, Agent Position: (1, 2), Performance: 49, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean [Clean] Clean \n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 18, Agent Position: (0, 2), Performance: 47, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n","[Clean] Clean  Clean \n"," Clean \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 19, Agent Position: (0, 3), Performance: 46, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n","[Clean]\n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 20, Agent Position: (0, 4), Performance: 44, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n","[Dirty] Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 21, Agent Position: (0, 4), Performance: 54, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n","[Clean] Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 22, Agent Position: (1, 4), Performance: 52, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean [Clean]\n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 23, Agent Position: (0, 4), Performance: 50, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n","[Clean] Clean \n"," Dirty  Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 24, Agent Position: (0, 5), Performance: 49, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n","[Dirty] Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 25, Agent Position: (0, 5), Performance: 59, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n","[Clean] Clean  Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 26, Agent Position: (1, 5), Performance: 57, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n"," Clean [Clean] Dirty \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 27, Agent Position: (2, 5), Performance: 56, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n"," Clean  Clean [Dirty]\n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 28, Agent Position: (2, 5), Performance: 66, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n"," Clean  Clean [Clean]\n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 29, Agent Position: (1, 5), Performance: 64, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n"," Clean [Clean] Clean \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 30, Agent Position: (1, 4), Performance: 62, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean [Clean]\n"," Clean  Clean  Clean \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 31, Agent Position: (0, 4), Performance: 61, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n","[Clean] Clean \n"," Clean  Clean  Clean \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 32, Agent Position: (0, 5), Performance: 60, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n","[Clean] Clean  Clean \n"," Dirty \n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 33, Agent Position: (0, 6), Performance: 59, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n"," Clean  Clean  Clean \n","[Dirty]\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 34, Agent Position: (0, 6), Performance: 69, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n"," Clean  Clean  Clean \n","[Clean]\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty \n","Step: 35, Agent Position: (0, 7), Performance: 67, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean \n","[Dirty] Dirty  Dirty  Dirty \n"," Dirty \n","Step: 36, Agent Position: (0, 7), Performance: 77, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean \n","[Clean] Dirty  Dirty  Dirty \n"," Dirty \n","Step: 37, Agent Position: (1, 7), Performance: 75, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean [Dirty] Dirty  Dirty \n"," Dirty \n","Step: 38, Agent Position: (1, 7), Performance: 85, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean [Clean] Dirty  Dirty \n"," Dirty \n","Step: 39, Agent Position: (2, 7), Performance: 84, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean [Dirty] Dirty \n"," Dirty \n","Step: 40, Agent Position: (2, 7), Performance: 94, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean [Clean] Dirty \n"," Dirty \n","Step: 41, Agent Position: (3, 7), Performance: 93, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean [Dirty]\n"," Dirty \n","Step: 42, Agent Position: (3, 7), Performance: 103, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean [Clean]\n"," Dirty \n","Step: 43, Agent Position: (2, 7), Performance: 101, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean [Clean] Clean \n"," Dirty \n","Step: 44, Agent Position: (1, 7), Performance: 98, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean [Clean] Clean  Clean \n"," Dirty \n","Step: 45, Agent Position: (0, 7), Performance: 96, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean \n","[Clean] Clean  Clean  Clean \n"," Dirty \n","Step: 46, Agent Position: (0, 8), Performance: 95, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean  Clean \n","[Dirty]\n","Step: 47, Agent Position: (0, 8), Performance: 105, Environment state:\n"," Clean  Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean \n"," Clean  Clean  Clean  Clean \n","[Clean]\n","Total steps: 48, Final performance: 105\n"]}],"source":["agent = ModelBasedReflexAgent()\n","environment = VacuumCleanerWorld([\n","    [\"Dirty\", \"Dirty\", \"Dirty\", \"Dirty\"],\n","    [\"Dirty\"],\n","    [\"Dirty\", \"Clean\", \"Dirty\"],\n","    [\"Clean\"],\n","    [\"Dirty\", \"Clean\"],\n","    [\"Dirty\", \"Clean\", \"Dirty\"],\n","    [\"Dirty\"],\n","    [\"Dirty\", \"Dirty\", \"Dirty\", \"Dirty\"],\n","    [\"Dirty\"]\n","])\n","\n","simulation = EnvironmentSimulator(agent, (0, 0), environment)\n","\n","total_steps = 48\n","for step in range(total_steps):\n","    simulation.step(step)\n","\n","simulation.agent_performance -= sum(space == \"Dirty\" for row in environment.dirt_placement for space in row) * 10\n","print(f\"Total steps: {total_steps}, Final performance: {simulation.agent_performance}\")"]},{"cell_type":"markdown","metadata":{"id":"goFe9Voif2Lj"},"source":["**Experiment 10:**\n","\n","In this scenario, the model-based agent skillfully maneuvers through a complex and irregular environment, navigating numerous turns and corners with efficiency, thoroughly cleaning all spaces, and ultimately securing an impressive final performance score of 105."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xSSFlBbCf2Lj","outputId":"85735842-cb1a-49fe-a1f8-07474f44656f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial Agent Position: (4, 5), Performance: 0, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty  Dirty \n","Step: 0, Agent Position: (3, 5), Performance: -1, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty [Dirty]\n","Step: 1, Agent Position: (3, 5), Performance: 9, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty [Clean]\n","Step: 2, Agent Position: (2, 5), Performance: 7, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty [Dirty] Clean \n","Step: 3, Agent Position: (2, 5), Performance: 17, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty [Clean] Clean \n","Step: 4, Agent Position: (1, 5), Performance: 16, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty [Dirty] Clean  Clean \n","Step: 5, Agent Position: (1, 5), Performance: 26, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Dirty [Clean] Clean  Clean \n","Step: 6, Agent Position: (0, 5), Performance: 25, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n","[Dirty] Clean  Clean  Clean \n","Step: 7, Agent Position: (0, 5), Performance: 35, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n","[Clean] Clean  Clean  Clean \n","Step: 8, Agent Position: (1, 5), Performance: 33, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty \n"," Dirty  Clean  Dirty \n"," Clean [Clean] Clean  Clean \n","Step: 9, Agent Position: (1, 4), Performance: 32, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty \n"," Dirty [Clean] Dirty \n"," Clean  Clean  Clean  Clean \n","Step: 10, Agent Position: (0, 4), Performance: 31, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty \n","[Dirty] Clean  Dirty \n"," Clean  Clean  Clean  Clean \n","Step: 11, Agent Position: (0, 4), Performance: 41, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty \n","[Clean] Clean  Dirty \n"," Clean  Clean  Clean  Clean \n","Step: 12, Agent Position: (1, 4), Performance: 39, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty \n"," Clean [Clean] Dirty \n"," Clean  Clean  Clean  Clean \n","Step: 13, Agent Position: (2, 4), Performance: 38, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty \n"," Clean  Clean [Dirty]\n"," Clean  Clean  Clean  Clean \n","Step: 14, Agent Position: (2, 4), Performance: 48, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty \n"," Clean  Clean [Clean]\n"," Clean  Clean  Clean  Clean \n","Step: 15, Agent Position: (1, 4), Performance: 46, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty  Dirty  Dirty \n"," Clean [Clean] Clean \n"," Clean  Clean  Clean  Clean \n","Step: 16, Agent Position: (1, 3), Performance: 45, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty [Dirty] Dirty \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 17, Agent Position: (1, 3), Performance: 55, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Dirty [Clean] Dirty \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 18, Agent Position: (0, 3), Performance: 54, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n","[Dirty] Clean  Dirty \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 19, Agent Position: (0, 3), Performance: 64, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n","[Clean] Clean  Dirty \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 20, Agent Position: (1, 3), Performance: 62, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Clean [Clean] Dirty \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 21, Agent Position: (2, 3), Performance: 61, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Clean  Clean [Dirty]\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 22, Agent Position: (2, 3), Performance: 71, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Clean  Clean [Clean]\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 23, Agent Position: (1, 3), Performance: 69, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty  Clean  Dirty \n"," Clean [Clean] Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 24, Agent Position: (1, 2), Performance: 68, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Dirty [Clean] Dirty \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 25, Agent Position: (0, 2), Performance: 67, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n","[Dirty] Clean  Dirty \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 26, Agent Position: (0, 2), Performance: 77, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n","[Clean] Clean  Dirty \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 27, Agent Position: (1, 2), Performance: 75, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Clean [Clean] Dirty \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 28, Agent Position: (2, 2), Performance: 74, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Clean  Clean [Dirty]\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 29, Agent Position: (2, 2), Performance: 84, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Clean  Clean [Clean]\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 30, Agent Position: (1, 2), Performance: 82, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty  Clean \n"," Clean [Clean] Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 31, Agent Position: (1, 1), Performance: 81, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Dirty [Clean]\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 32, Agent Position: (0, 1), Performance: 80, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n","[Dirty] Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 33, Agent Position: (0, 1), Performance: 90, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n","[Clean] Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 34, Agent Position: (1, 1), Performance: 88, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n"," Clean [Clean]\n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 35, Agent Position: (0, 1), Performance: 86, Environment state:\n"," Dirty  Dirty  Dirty  Dirty \n","[Clean] Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 36, Agent Position: (0, 0), Performance: 85, Environment state:\n","[Dirty] Dirty  Dirty  Dirty \n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 37, Agent Position: (0, 0), Performance: 95, Environment state:\n","[Clean] Dirty  Dirty  Dirty \n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 38, Agent Position: (1, 0), Performance: 93, Environment state:\n"," Clean [Dirty] Dirty  Dirty \n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 39, Agent Position: (1, 0), Performance: 103, Environment state:\n"," Clean [Clean] Dirty  Dirty \n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 40, Agent Position: (2, 0), Performance: 102, Environment state:\n"," Clean  Clean [Dirty] Dirty \n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 41, Agent Position: (2, 0), Performance: 112, Environment state:\n"," Clean  Clean [Clean] Dirty \n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 42, Agent Position: (3, 0), Performance: 111, Environment state:\n"," Clean  Clean  Clean [Dirty]\n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Step: 43, Agent Position: (3, 0), Performance: 121, Environment state:\n"," Clean  Clean  Clean [Clean]\n"," Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean \n"," Clean  Clean  Clean  Clean \n","Total steps: 44, Final performance: 121\n"]}],"source":["agent = ModelBasedReflexAgent()\n","environment = VacuumCleanerWorld([\n","    [\"Dirty\", \"Dirty\", \"Dirty\", \"Dirty\"],\n","    [\"Dirty\", \"Clean\"],\n","    [\"Dirty\", \"Clean\", \"Dirty\"],\n","    [\"Dirty\", \"Dirty\", \"Dirty\"],\n","    [\"Dirty\", \"Clean\", \"Dirty\"],\n","    [\"Dirty\", \"Dirty\", \"Dirty\", \"Dirty\"],\n","])\n","\n","simulation = EnvironmentSimulator(agent, (4, 5), environment)\n","\n","total_steps = 44\n","for step in range(total_steps):\n","    simulation.step(step)\n","\n","simulation.agent_performance -= sum(space == \"Dirty\" for row in environment.dirt_placement for space in row) * 10\n","print(f\"Total steps: {total_steps}, Final performance: {simulation.agent_performance}\")"]},{"cell_type":"markdown","metadata":{"id":"A0BBnuaGf2Lj"},"source":["**Experiment 11:**\n","\n","In this concluding scenario, the model-based agent demonstrates its adeptness once more by navigating a vast and irregular environment, effectively cleaning every space despite starting from a distant, enclosed corner. This adept maneuvering leads to a commendable final score of 121."]},{"cell_type":"markdown","metadata":{"id":"F03o9jC_f2Lj"},"source":["## III. Analysis\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In the project's initial phase, a modular simulator was designed to measure the performance of vacuum cleaner agents within various environments. During **Experiments 1 & 2**, the behavior of a simple reflex agent was examined as it navigated linear environments. A key observation was the agent's inability to recall previously visited areas, resulting in repetitive back-and-forth movements. To mitigate this inefficiency, the simulation was prematurely terminated, resulting in scores of 19 and 28 for **Experiments 1 & 2**, respectively.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In the project's second phase, a more complex environment simulator for measuring performance was developed, enabling 2D movement. **Experiments 3 & 4** contrasted a simple reflex agent against a randomized version, revealing the former's tendency becomes entrapped in repetitive loops and scoring -95, while the latter explored the environment fully and scored 17. The simple reflex agent's design, based on condition-action rules [1], limits its effectiveness in new environments, often resulting in repetitive loops and incomplete exploration. In contrast, adding randomness to the decision-making process allows for more thorough navigation, as demonstrated in various simulations. **Experiments 5 & 6** tested these agents in altered environments and starting positions, with the simple reflex agent slightly improving but still underperforming with a score of -18. However, the randomized agent showed better exploration capabilities, achieving a score of 71. Yet, its reliance on chance limits efficiency in larger spaces or without ample exploration time. In **Experiments 7 & 8**, under constrained conditions, the randomized agent failed to clean effectively, scoring -63, whereas the simple reflex agent excelled in the large, regular environment with a score of 111. Subsequent experiments introduced a model-based agent with an internal state, significantly outperforming the simple reflex agent by dynamically updating the environment's state and planning actions more effectively [1]. This agent type is fundamentally rational, leveraging its set rules and internal status to select actions aimed at enhancing its performance metrics. This approach led to high scores in navigating and cleaning large, complex environments, with scores of 172, 105, and 121 in **Experiments 9, 10, and 11**, respectively, underscoring the model-based agent's superior exploration and cleaning efficiency.\n"]},{"cell_type":"markdown","metadata":{"id":"wexWENjbf2Lj"},"source":["## VI. Conclusion\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In the exploration of artificial intelligence and robotics, distinguishing between the capabilities of different agent designs is essential for effective problem-solving. Simple reflex agents, constrained by their condition-action rule framework, often struggle in unfamiliar environments, getting trapped in repetitive loops that hinder comprehensive exploration. This limitation points to the necessity of developing agents capable of adapting to various environmental conditions without succumbing to inefficiency. In contrast, incorporating randomness into a reflex agent's decision-making process offers a potential solution to the rigidity of rule-based systems. By enabling non-deterministic choices, agents can achieve more thorough exploration over time, albeit with effectiveness dependent on environmental size and exploration duration. This approach underscores a crucial trade-off between the unpredictability of random decisions and the structured planning integral to agent design.\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The advancement to model-based reflex agents, which maintain a dynamic understanding of their environment and their position within it, represents a significant leap forward. These agents can make informed decisions that optimize their actions towards performance, combining adaptability with systematic exploration to ensure efficient and comprehensive environmental interaction. For future computer engineers, selecting the right type of agent is critical, directly influencing the effectiveness of developed solutions. Understanding the limitations of simple reflex agents, the exploratory potential of randomness, and the superior adaptability of model-based agents provides a solid foundation for designing intelligent systems."]},{"cell_type":"markdown","metadata":{"id":"GNtVlkJSf2Lj"},"source":["# References\n","\n","1. Russell, S. J. (2016). Artificial Intelligence: A modern approach. Pearson.\n","2. Aimacode. Aimacode/aima-python: Python implementation of algorithms from Russell and Norvig’s “artificial intelligence - A modern approach.” GitHub. https://github.com/aimacode/aima-python\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}